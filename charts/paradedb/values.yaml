installOperator: true
installUI: true

apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: paradedb-cluster  # Name of the PostgreSQL cluster resource
  # labels: # Labels for the PostgreSQL cluster resource
  #   application: test-app
  #   environment: demo
  # annotations:  # Annotations for the PostgreSQL cluster resource
  #   "acid.zalan.do/controller": "second-operator"  # Specify a different operator to control this cluster
  #   "delete-date": "2020-08-31"  # Cluster can only be deleted on this date if configured
  #   "delete-clustername": "acid-test-cluster"  # Cluster can only be deleted when the name matches if configured

spec:
  dockerImage: ghcr.io/zalando/spilo-15:3.0-p1  # Docker image to use for the PostgreSQL cluster
  # teamId: "paradedb"  # Team responsible for the cluster
  numberOfInstances: 2  # Number of PostgreSQL instances (includes master and replicas)

  users:  # Application/Robot users
    paradedb:
    - superuser  # PostgreSQL superuser role
    - createdb  # Role that allows creation of databases
#   foo_user: []  # Empty list means no specific roles
#    flyway: []
#  usersWithSecretRotation:
#  - foo_user
#  usersWithInPlaceSecretRotation:
#  - flyway
#  - bar_owner_user

  enableMasterLoadBalancer: false  # If true, a load balancer for the master is created
  enableReplicaLoadBalancer: false  # If true, a load balancer for the replicas is created
  enableConnectionPooler: false  # Enable/disable connection pooler deployment
  enableReplicaConnectionPooler: false  # Enable connection pooler for replica service
  enableMasterPoolerLoadBalancer: false  # Enable load balancer for master connection pooler
  enableReplicaPoolerLoadBalancer: false  # Enable load balancer for replica connection pooler

  allowedSourceRanges:  # Allowed IP ranges for master and replica services
  - 127.0.0.1/32

  databases:
    paradedb: paradedb  # Database 'foo' with owner 'zalando'

  preparedDatabases:  # Pre-defined databases and their configurations
    paradedb:
      defaultUsers: true
      extensions:
        pg_partman: public
        pgcrypto: public
      schemas:
        data: {}
        history:
          defaultRoles: true
          defaultUsers: false

  postgresql:
    version: "15"  # PostgreSQL version to use
    parameters:  # Custom PostgreSQL parameters
      shared_buffers: "32MB"
      max_connections: "10"
      log_statement: "all"
#  env:
#  - name: wal_s3_bucket
#    value: my-custom-bucket

  volume:
    size: 10Gi  # Size of the volume for the PostgreSQL instance
#    storageClass: my-sc
#    iops: 1000  # for EBS gp3
#    throughput: 250  # in MB/s for EBS gp3
#    selector:
#      matchExpressions:
#        - { key: flavour, operator: In, values: [ "banana", "chocolate" ] }
#      matchLabels:
#        environment: dev
#        service: postgres

  additionalVolumes:  # Additional volumes to be mounted to the PostgreSQL pod
    - name: empty
      mountPath: /opt/empty  # Mount path inside the container
      targetContainers:
        - all  # Target container to mount the volume. 'all' means all containers in the pod.
      volumeSource:
        emptyDir: {}  # Type of the volume. Here, it's an empty directory.
#    - name: data
#      mountPath: /home/postgres/pgdata/partitions
#      targetContainers:
#        - postgres
#      volumeSource:
#        PersistentVolumeClaim:
#          claimName: pvc-postgresql-data-partitions
#          readyOnly: false
#    - name: conf
#      mountPath: /etc/telegraf
#      subPath: telegraf.conf
#      targetContainers:
#        - telegraf-sidecar
#      volumeSource:
#        configMap:
#          name: my-config-map

  enableShmVolume: true  # Enables shared memory volume
#  spiloRunAsUser: 101
#  spiloRunAsGroup: 103
#  spiloFSGroup: 103
#  podAnnotations:
#    annotation.key: value
#  serviceAnnotations:
#    annotation.key: value
#  podPriorityClassName: "spilo-pod-priority"
#  tolerations:
#  - key: postgres
#    operator: Exists
#    effect: NoSchedule

  resources:  # Resource requests and limits for the PostgreSQL pod
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi

  patroni:  # Configuration for the Patroni PostgreSQL manager
    failsafe_mode: false
    initdb:  # Initial database configuration
      encoding: "UTF8"
      locale: "en_US.UTF-8"
      data-checksums: "true"
    pg_hba:
      - hostssl all all 0.0.0.0/0 md5
      - host    all all 0.0.0.0/0 md5
#    slots:
#      permanent_physical_1:
#        type: physical
#      permanent_logical_1:
#        type: logical
#        database: foo
#        plugin: pgoutput
    ttl: 30  # Time (in seconds) to live for the leader key in DCS (Distributed Configuration Store)
    loop_wait: 10  # Delay (in seconds) between iterations for Patroni
    retry_timeout: 10  # Retry timeout value (in seconds) for Patroni
    synchronous_mode: false  # Enables or disables synchronous mode for Patroni
    synchronous_mode_strict: false  # If true, synchronous_mode does not change automatically
    synchronous_node_count: 1  # Number of nodes that need to be in sync
    maximum_lag_on_failover: 33554432  # Max lag size on failover

# restore a Postgres DB with point-in-time-recovery
# with a non-empty timestamp, clone from an S3 bucket using the latest backup before the timestamp
# with an empty/absent timestamp, clone from an existing alive cluster using pg_basebackup
#  clone:
#    uid: "efd12e58-5786-11e8-b5a7-06148230260c"
#    cluster: "acid-minimal-cluster"
#    timestamp: "2017-12-19T12:40:33+01:00"  # timezone required (offset relative to UTC, see RFC 3339 section 5.6)
#    s3_wal_path: "s3://custom/path/to/bucket"

# run periodic backups with k8s cron jobs
#  enableLogicalBackup: true
#  logicalBackupSchedule: "30 00 * * *"

#  maintenanceWindows:
#  - 01:00-06:00  #UTC
#  - Sat:00:00-04:00

# overwrite custom properties for connection pooler deployments
#  connectionPooler:
#    numberOfInstances: 2
#    mode: "transaction"
#    schema: "pooler"
#    user: "pooler"
#    maxDBConnections: 60
#    resources:
#      requests:
#        cpu: 300m
#        memory: 100Mi
#      limits:
#        cpu: "1"
#        memory: 100Mi

#   initContainers:
#   - name: date
#     image: busybox
#     command: [ "/bin/date" ]
#  sidecars:
#   - name: "telegraf-sidecar"
#     image: "telegraf:latest"
#     ports:
#     - name: metrics
#       containerPort: 8094
#       protocol: TCP
#     resources:
#       limits:
#         cpu: 500m
#         memory: 500Mi
#       requests:
#         cpu: 100m
#         memory: 100Mi
#     env:
#       - name: "USEFUL_VAR"
#         value: "perhaps-true"

# Custom TLS certificate. Disabled unless tls.secretName has a value.
  tls:
    secretName: ""  # should correspond to a Kubernetes Secret resource to load
    certificateFile: "tls.crt"
    privateKeyFile: "tls.key"
    caFile: ""  # optionally configure Postgres with a CA certificate
    caSecretName: "" # optionally the ca.crt can come from this secret instead.

# file names can be also defined with absolute path, and will no longer be relative
# to the "/tls/" path where the secret is being mounted by default, and "/tlsca/"
# where the caSecret is mounted by default.
# When TLS is enabled, also set spiloFSGroup parameter above to the relevant value.
# if unknown, set it to 103 which is the usual value in the default spilo images.
# In Openshift, there is no need to set spiloFSGroup/spilo_fsgroup.

# Add node affinity support by allowing postgres pods to schedule only on nodes that
# have label: "postgres-operator:enabled" set.
#  nodeAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      nodeSelectorTerms:
#        - matchExpressions:
#            - key: postgres-operator
#              operator: In
#              values:
#                - enabled

# Enables change data capture streams for defined database tables
#  streams:
#  - applicationId: test-app
#    database: foo
#    tables:
#      data.state_pending_outbox:
#        eventType: test-app.status-pending
#      data.state_approved_outbox:
#        eventType: test-app.status-approved
#      data.orders_outbox:
#        eventType: test-app.order-completed
#        idColumn: o_id
#        payloadColumn: o_payload
#    # Optional. Filter ignores events before a certain txnId and lsn. Can be used to skip bad events
#    filter:
#      data.orders_outbox: "[?(@.source.txId > 500 && @.source.lsn > 123456)]"
#    batchSize: 1000